models:
  base:
    name: "meta-llama/Llama-2-7b-chat-hf"
    tokenizer: "meta-llama/Llama-2-7b-chat-hf"
    device: "cuda"
    device_map: "auto"
    dtype: "float16"
  
  unlearned:
    name: "microsoft/Llama2-7b-WhoIsHarryPotter"
    tokenizer: "meta-llama/Llama-2-7b-chat-hf"
    device: "cuda"
    device_map: "auto"
    dtype: "float16"

layers:
  start: 8
  end: 31

datasets:
  harry_potter:
    num_queries: 5
    source: "muse-bench/MUSE-Books"
    subset: "knowmem"
  tofu:
    num_queries: 0
  wmdp:
    num_queries: 0

geometric_features:
  - local_density
  - separability
  - centrality
  - cross_layer_consistency
  - isolation
  - cluster_compactness

feature_params:
  k_neighbors: 3
  pca_components: 3

attack:
  method: "prompt"  # Change to "logit_lens" for the other test
  
  prompt:
    domain: "harry_potter"
    samples_per_strategy: 2
  
  logit_lens:
    top_k: 10
  
  num_samples: 5
  temperature: 1.0
  top_k: 40
  max_new_tokens: 30

prediction:
  models:
    - linear_regression
    - random_forest
  cv_folds: 3
  test_size: 0.2

output:
  save_activations: true
  save_features: true
  save_predictions: true
  create_visualizations: false
  output_dir: "./outputs"